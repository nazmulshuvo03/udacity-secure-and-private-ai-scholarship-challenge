{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, ), (0.5,))])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of teachers to essemble\n",
    "num_teachers = 100\n",
    "\n",
    "def get_data_loaders(train_data, num_teachers = 10):\n",
    "    teacher_loaders = []\n",
    "    data_size = len(train_data) // num_teachers\n",
    "\n",
    "    for i in range(num_teachers):\n",
    "        indices = list(range(i*data_size, (i+1) *data_size))\n",
    "        subset_data = Subset(train_data, indices)\n",
    "        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size)\n",
    "        teacher_loaders.append(loader)\n",
    "\n",
    "    return teacher_loaders\n",
    "\n",
    "teacher_loaders = get_data_loaders(train_data, num_teachers)\n",
    "len(teacher_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_train_data = Subset(test_data, list(range(9000)))\n",
    "student_test_data = Subset(test_data, list(range(9000, 10000)))\n",
    "\n",
    "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)\n",
    "student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)\n",
    "\n",
    "len(student_train_loader), len(student_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, epochs=10, print_every=120):\n",
    "    model.to(device)\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            steps += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model.forward(images)\n",
    "        ps = torch.argmax(torch.exp(output), dim=1)\n",
    "        outputs = torch.cat((outputs, ps))\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training teacher 1\n",
      "Training teacher 2\n",
      "Training teacher 3\n",
      "Training teacher 4\n",
      "Training teacher 5\n",
      "Training teacher 6\n",
      "Training teacher 7\n",
      "Training teacher 8\n",
      "Training teacher 9\n",
      "Training teacher 10\n",
      "Training teacher 11\n",
      "Training teacher 12\n",
      "Training teacher 13\n",
      "Training teacher 14\n",
      "Training teacher 15\n",
      "Training teacher 16\n",
      "Training teacher 17\n",
      "Training teacher 18\n",
      "Training teacher 19\n",
      "Training teacher 20\n",
      "Training teacher 21\n",
      "Training teacher 22\n",
      "Training teacher 23\n",
      "Training teacher 24\n",
      "Training teacher 25\n",
      "Training teacher 26\n",
      "Training teacher 27\n",
      "Training teacher 28\n",
      "Training teacher 29\n",
      "Training teacher 30\n",
      "Training teacher 31\n",
      "Training teacher 32\n",
      "Training teacher 33\n",
      "Training teacher 34\n",
      "Training teacher 35\n",
      "Training teacher 36\n",
      "Training teacher 37\n",
      "Training teacher 38\n",
      "Training teacher 39\n",
      "Training teacher 40\n",
      "Training teacher 41\n",
      "Training teacher 42\n",
      "Training teacher 43\n",
      "Training teacher 44\n",
      "Training teacher 45\n",
      "Training teacher 46\n",
      "Training teacher 47\n",
      "Training teacher 48\n",
      "Training teacher 49\n",
      "Training teacher 50\n",
      "Training teacher 51\n",
      "Training teacher 52\n",
      "Training teacher 53\n",
      "Training teacher 54\n",
      "Training teacher 55\n",
      "Training teacher 56\n",
      "Training teacher 57\n",
      "Training teacher 58\n",
      "Training teacher 59\n",
      "Training teacher 60\n",
      "Training teacher 61\n",
      "Training teacher 62\n",
      "Training teacher 63\n",
      "Training teacher 64\n",
      "Training teacher 65\n",
      "Training teacher 66\n",
      "Training teacher 67\n",
      "Training teacher 68\n",
      "Training teacher 69\n",
      "Training teacher 70\n",
      "Training teacher 71\n",
      "Training teacher 72\n",
      "Training teacher 73\n",
      "Training teacher 74\n",
      "Training teacher 75\n",
      "Training teacher 76\n",
      "Training teacher 77\n",
      "Training teacher 78\n",
      "Training teacher 79\n",
      "Training teacher 80\n",
      "Training teacher 81\n",
      "Training teacher 82\n",
      "Training teacher 83\n",
      "Training teacher 84\n",
      "Training teacher 85\n",
      "Training teacher 86\n",
      "Training teacher 87\n",
      "Training teacher 88\n",
      "Training teacher 89\n",
      "Training teacher 90\n",
      "Training teacher 91\n",
      "Training teacher 92\n",
      "Training teacher 93\n",
      "Training teacher 94\n",
      "Training teacher 95\n",
      "Training teacher 96\n",
      "Training teacher 97\n",
      "Training teacher 98\n",
      "Training teacher 99\n",
      "Training teacher 100\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the models for each teacher\n",
    "def train_models(num_teachers):\n",
    "    models = []\n",
    "    for t in range(num_teachers):\n",
    "        print(\"Training teacher {}\".format(t+1))\n",
    "        model = Network()\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "        train(model, teacher_loaders[t], criterion, optimizer)\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "models = train_models(num_teachers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epsilon = 0.2\n",
    "\n",
    "def aggregated_teacher(models, data_loader, epsilon):\n",
    "    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)\n",
    "    for i, model in enumerate(models):\n",
    "        results = predict(model, data_loader)\n",
    "        preds[i] = results\n",
    "        \n",
    "    labels = np.array([]).astype(int)\n",
    "    for image_preds in np.transpose(preds):\n",
    "        label_counts = np.bincount(image_preds, minlength=10)\n",
    "        beta = 1 / epsilon\n",
    "\n",
    "        for i in range(len(label_counts)):\n",
    "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "\n",
    "        new_label = np.argmax(label_counts)\n",
    "        labels = np.append(labels, new_label)\n",
    "    \n",
    "    return preds.numpy(), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_models = models\n",
    "preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 9000), array([7, 2, 1, ..., 6, 9, 0]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, student_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 1451.5129254649705\n",
      "Data Dependent Epsilon: 6.689155555034445\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_loader(student_train_loader, labels):\n",
    "    for i, (data, _) in enumerate(iter(student_train_loader)):\n",
    "        yield data, torch.from_numpy(labels[i*len(data):(i+1)*len(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.398..  Test Loss: 2.020..  Test Accuracy: 0.349\n",
      "Epoch: 1/10..  Training Loss: 0.276..  Test Loss: 0.750..  Test Accuracy: 0.829\n",
      "Epoch: 1/10..  Training Loss: 0.159..  Test Loss: 0.455..  Test Accuracy: 0.879\n",
      "Epoch: 1/10..  Training Loss: 0.105..  Test Loss: 0.368..  Test Accuracy: 0.896\n",
      "Epoch: 1/10..  Training Loss: 0.099..  Test Loss: 0.275..  Test Accuracy: 0.928\n",
      "Epoch: 2/10..  Training Loss: 0.120..  Test Loss: 0.325..  Test Accuracy: 0.920\n",
      "Epoch: 2/10..  Training Loss: 0.098..  Test Loss: 0.264..  Test Accuracy: 0.927\n",
      "Epoch: 2/10..  Training Loss: 0.086..  Test Loss: 0.231..  Test Accuracy: 0.937\n",
      "Epoch: 2/10..  Training Loss: 0.075..  Test Loss: 0.218..  Test Accuracy: 0.937\n",
      "Epoch: 2/10..  Training Loss: 0.064..  Test Loss: 0.210..  Test Accuracy: 0.942\n",
      "Epoch: 2/10..  Training Loss: 0.060..  Test Loss: 0.202..  Test Accuracy: 0.939\n",
      "Epoch: 3/10..  Training Loss: 0.089..  Test Loss: 0.203..  Test Accuracy: 0.944\n",
      "Epoch: 3/10..  Training Loss: 0.073..  Test Loss: 0.189..  Test Accuracy: 0.945\n",
      "Epoch: 3/10..  Training Loss: 0.063..  Test Loss: 0.198..  Test Accuracy: 0.944\n",
      "Epoch: 3/10..  Training Loss: 0.049..  Test Loss: 0.181..  Test Accuracy: 0.946\n",
      "Epoch: 3/10..  Training Loss: 0.046..  Test Loss: 0.197..  Test Accuracy: 0.945\n",
      "Epoch: 4/10..  Training Loss: 0.068..  Test Loss: 0.186..  Test Accuracy: 0.944\n",
      "Epoch: 4/10..  Training Loss: 0.060..  Test Loss: 0.195..  Test Accuracy: 0.939\n",
      "Epoch: 4/10..  Training Loss: 0.061..  Test Loss: 0.190..  Test Accuracy: 0.944\n",
      "Epoch: 4/10..  Training Loss: 0.061..  Test Loss: 0.191..  Test Accuracy: 0.947\n",
      "Epoch: 4/10..  Training Loss: 0.039..  Test Loss: 0.173..  Test Accuracy: 0.952\n",
      "Epoch: 4/10..  Training Loss: 0.041..  Test Loss: 0.171..  Test Accuracy: 0.954\n",
      "Epoch: 5/10..  Training Loss: 0.084..  Test Loss: 0.177..  Test Accuracy: 0.949\n",
      "Epoch: 5/10..  Training Loss: 0.060..  Test Loss: 0.167..  Test Accuracy: 0.952\n",
      "Epoch: 5/10..  Training Loss: 0.048..  Test Loss: 0.177..  Test Accuracy: 0.952\n",
      "Epoch: 5/10..  Training Loss: 0.047..  Test Loss: 0.175..  Test Accuracy: 0.951\n",
      "Epoch: 5/10..  Training Loss: 0.039..  Test Loss: 0.188..  Test Accuracy: 0.948\n",
      "Epoch: 5/10..  Training Loss: 0.037..  Test Loss: 0.175..  Test Accuracy: 0.951\n",
      "Epoch: 6/10..  Training Loss: 0.080..  Test Loss: 0.166..  Test Accuracy: 0.947\n",
      "Epoch: 6/10..  Training Loss: 0.051..  Test Loss: 0.171..  Test Accuracy: 0.950\n",
      "Epoch: 6/10..  Training Loss: 0.052..  Test Loss: 0.178..  Test Accuracy: 0.948\n",
      "Epoch: 6/10..  Training Loss: 0.044..  Test Loss: 0.170..  Test Accuracy: 0.952\n",
      "Epoch: 6/10..  Training Loss: 0.036..  Test Loss: 0.170..  Test Accuracy: 0.949\n",
      "Epoch: 7/10..  Training Loss: 0.076..  Test Loss: 0.175..  Test Accuracy: 0.956\n",
      "Epoch: 7/10..  Training Loss: 0.056..  Test Loss: 0.167..  Test Accuracy: 0.951\n",
      "Epoch: 7/10..  Training Loss: 0.052..  Test Loss: 0.170..  Test Accuracy: 0.953\n",
      "Epoch: 7/10..  Training Loss: 0.054..  Test Loss: 0.176..  Test Accuracy: 0.949\n",
      "Epoch: 7/10..  Training Loss: 0.035..  Test Loss: 0.175..  Test Accuracy: 0.953\n",
      "Epoch: 7/10..  Training Loss: 0.031..  Test Loss: 0.170..  Test Accuracy: 0.952\n",
      "Epoch: 8/10..  Training Loss: 0.070..  Test Loss: 0.170..  Test Accuracy: 0.952\n",
      "Epoch: 8/10..  Training Loss: 0.050..  Test Loss: 0.158..  Test Accuracy: 0.955\n",
      "Epoch: 8/10..  Training Loss: 0.046..  Test Loss: 0.165..  Test Accuracy: 0.954\n",
      "Epoch: 8/10..  Training Loss: 0.042..  Test Loss: 0.171..  Test Accuracy: 0.949\n",
      "Epoch: 8/10..  Training Loss: 0.032..  Test Loss: 0.167..  Test Accuracy: 0.954\n",
      "Epoch: 8/10..  Training Loss: 0.031..  Test Loss: 0.163..  Test Accuracy: 0.955\n",
      "Epoch: 9/10..  Training Loss: 0.086..  Test Loss: 0.156..  Test Accuracy: 0.954\n",
      "Epoch: 9/10..  Training Loss: 0.050..  Test Loss: 0.163..  Test Accuracy: 0.956\n",
      "Epoch: 9/10..  Training Loss: 0.050..  Test Loss: 0.163..  Test Accuracy: 0.952\n",
      "Epoch: 9/10..  Training Loss: 0.032..  Test Loss: 0.166..  Test Accuracy: 0.953\n",
      "Epoch: 9/10..  Training Loss: 0.033..  Test Loss: 0.157..  Test Accuracy: 0.955\n",
      "Epoch: 10/10..  Training Loss: 0.057..  Test Loss: 0.153..  Test Accuracy: 0.961\n",
      "Epoch: 10/10..  Training Loss: 0.052..  Test Loss: 0.160..  Test Accuracy: 0.955\n",
      "Epoch: 10/10..  Training Loss: 0.039..  Test Loss: 0.160..  Test Accuracy: 0.954\n",
      "Epoch: 10/10..  Training Loss: 0.043..  Test Loss: 0.168..  Test Accuracy: 0.950\n",
      "Epoch: 10/10..  Training Loss: 0.029..  Test Loss: 0.160..  Test Accuracy: 0.952\n",
      "Epoch: 10/10..  Training Loss: 0.029..  Test Loss: 0.172..  Test Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "student_model = Network()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "student_model.to(device)\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "for e in range(epochs):\n",
    "    # Model in training mode, dropout is on\n",
    "    student_model.train()\n",
    "    train_loader = student_loader(student_train_loader, student_labels)\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = student_model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if steps % 50 == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            student_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for images, labels in student_test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    log_ps = student_model(images)\n",
    "                    test_loss += criterion(log_ps, labels).item()\n",
    "                    \n",
    "                    # Accuracy\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            student_model.train()\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/len(student_train_loader)),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(student_test_loader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(student_test_loader)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 11.291..  Test Accuracy: 28.812\n"
     ]
    }
   ],
   "source": [
    "t1_model = models[99]\n",
    "t1_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in student_test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        log_ps = t1_model(images)\n",
    "        test_loss += criterion(log_ps, labels).item()\n",
    "\n",
    "        # Accuracy\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    t1_model.train()\n",
    "    print(\"Test Loss: {:.3f}.. \".format(test_loss),\n",
    "          \"Test Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
