{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.to(device)\n",
    "\n",
    "    # train the model\n",
    "    epochs = epochs\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        accuracy=0\n",
    "        running_loss = 0\n",
    "        for images, labels in dataloader:\n",
    "            steps += 1\n",
    "\n",
    "            # Move input and label tensors to the GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataLoader, model):  \n",
    "    outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
    "    model.to(device)\n",
    "    # Model in test mode, dropout is off\n",
    "    model.eval()\n",
    "    result=[]\n",
    "    for images, labels in dataLoader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model.forward(images)\n",
    "        ps = torch.argmax(torch.exp(output), dim=1)\n",
    "        outputs = torch.cat((outputs, ps))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, ), (0.5,))])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "device =  torch.device(\"cuda:0\"\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_teachers = 100\n",
    "train_len = len(mnist_trainset)\n",
    "test_len = len(mnist_testset)\n",
    "train_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs=1\n",
    "print_every=120\n",
    "split= int(train_len/num_teachers)\n",
    "split_ini = split\n",
    "indices = list(range(train_len))\n",
    "init=0\n",
    "samples = []\n",
    "teachers = []\n",
    "\n",
    "for i in range(num_teachers):     \n",
    "    t_idx = indices[init:split]\n",
    "    t_sampler = SubsetRandomSampler(t_idx)\n",
    "    samples.append(t_sampler)\n",
    "    init = split\n",
    "    split = split+split_ini\n",
    "    \n",
    "for sample in samples:\n",
    "    loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=32, sampler=sample)\n",
    "    teacher = train(Classifier(), loader, epochs)\n",
    "    teachers.append(teacher)\n",
    "\n",
    "teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.torch.zeros((num_teachers, test_len), dtype=torch.long)\n",
    "for key, teacher in enumerate(teachers):\n",
    "    test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=32)\n",
    "    preds[key] = eval(test_loader, Classifier())\n",
    "    \n",
    "preds = preds.numpy()\n",
    "preds.shape, type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "def get_labels(preds, epsilon):\n",
    "    beta = 1 / epsilon\n",
    "    labels = np.array([]).astype(int)\n",
    "    for pred in np.transpose(preds):\n",
    "        label_counts = np.bincount(pred, minlength=10)\n",
    "        for i in range(len(label_counts)):\n",
    "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "        \n",
    "        new_label = np.argmax(label_counts)\n",
    "        labels = np.append(labels, new_label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(preds, epsilon)\n",
    "labels.shape, type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=labels, noise_eps=epsilon, delta=1e-5)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
